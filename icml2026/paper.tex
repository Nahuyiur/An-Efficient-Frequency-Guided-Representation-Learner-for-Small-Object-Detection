%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage[table]{xcolor}
\usepackage{multirow}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\mapcell}[2]{\makebox[5.2em][l]{#1\if\relax\detokenize{#2}\relax\else\hspace{0.25em}{\fontsize{3.2pt}{3.2pt}\selectfont\textcolor{black}{(}\textcolor{red}{+#2}\textcolor{black}{)}}\fi}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% For preprint, use
% \usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2026}

\begin{document}

\twocolumn[
  \icmltitle{From Spatial to Spectral: An Efficient, Frequency-Guided Representation Learner for Small Object Detection}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Firstname1 Lastname1}{equal,yyy}
    \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
    \icmlauthor{Firstname3 Lastname3}{comp}
    \icmlauthor{Firstname4 Lastname4}{sch}
    \icmlauthor{Firstname5 Lastname5}{yyy}
    \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    \icmlauthor{Firstname8 Lastname8}{sch}
    \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
  \end{icmlauthorlist}

  \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
  \icmlaffiliation{comp}{Company Name, Location, Country}
  \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

  \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
  \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{Machine Learning, ICML}

  \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.
% If you have no special notice, KEEP empty braces:
\printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
% \printAffiliationsAndNotice{\icmlEqualContribution}



%---------------------
\begin{abstract}
  This document provides a basic paper template and submission guidelines.
  Abstracts must be a single paragraph, ideally between 4--6 sentences long.
  Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

%---------------------
\section{Introduction}
%注意,这个地方introduction基本上是瞎写的,完全没有结合论文内容
Small object detection remains a significant challenge due to the limited spatial information they occupy in images. These objects are dominated by high-frequency cues such as edges and fine textures. However, modern detectors face three key issues: (i) Backbone downsampling, which acts as a low-pass filter, reduces these high-frequency components; (ii) Multi-scale fusion in the neck further smooths the boundaries; and (iii) The head's regression gradients weaken once boundary evidence is diluted during the detection process. These problems make small-object localization particularly difficult.

To address these challenges, we propose a frequency-guided solution that operates on the principle of \emph{decomposing features into low and high-frequency components, selectively enhancing them, and reconstructing or injecting the enhanced signal back into the feature stream}. This approach, encapsulated in the \textbf{Frequency-guided Decompose--Enhance--Reconstruct (DER) operator}, enables us to effectively preserve, enhance, and exploit high-frequency details across the detection pipeline.

Building on DER, we introduce three lightweight, plug-and-play components that can be inserted into the backbone, neck, and head, respectively.

These modules form a coherent frequency flow across the backbone, neck, and head, addressing the challenges posed by small-object detection with minimal computational overhead. Our method is generalizable across different detector architectures, and we demonstrate its effectiveness through extensive experiments across multiple benchmarks.

\noindent\textbf{Contributions:} We make the following contributions:
\begin{itemize}
    \item We propose the DER operator, which integrates frequency-domain decomposition, enhancement, and reconstruction in a unified framework.
    \item We introduce three novel modules (WDG, LGE, FDHead) that instantiate the DER operator at key locations in the detection pipeline.
    \item We show that our method improves small-object detection performance while maintaining computational efficiency, and we validate its generalizability across different architectures.
\end{itemize}



%---------------------
\section{Related Work}

We review prior work from three angles that are most relevant to our goal: (i) efficient detector architectures, (ii) small object detection strategies, and (iii) frequency-domain modeling for dense prediction.

\subsection{Efficient Detector Architectures}

Real-time detection has been driven by architectural efficiency in backbones, feature pyramids, and heads. One-stage YOLO-style detectors optimize the accuracy--latency trade-off through carefully designed blocks and multi-scale prediction, with recent variants continuing to improve both speed and accuracy \cite{yolov10,yolov11}. Lightweight enhancements for challenging regimes (e.g., cluttered scenes) often rely on stronger feature aggregation or multi-kernel perception to increase representational diversity while keeping inference efficient \cite{fbrt-yolo}. 

In parallel, Transformer-based detectors seek end-to-end set prediction by removing hand-crafted components such as anchors \cite{detr}. Subsequent work improves the practicality of DETR-like models via more efficient attention and training strategies, enabling competitive performance under constrained budgets \cite{rt-detr,dino}. Despite these advances, both CNN- and Transformer-based detectors still face a common tension for tiny/small objects: improving fine-detail sensitivity typically increases computation, memory, or architectural intrusion, making it difficult to deploy a uniformly effective solution across detector families.

\subsection{Small Object Detection}

Small objects are inherently information-limited: they occupy few pixels, induce weak feature responses, and are easily suppressed by downsampling and coarse fusion. Early two-stage and one-stage frameworks (e.g., Faster R-CNN and SSD) already revealed the difficulty of preserving small-object cues under feature hierarchy and stride growth \cite{fasterrcnn,ssd}. A large body of work improves small-object performance by strengthening multi-scale feature fusion (e.g., FPN and its variants) \cite{fpn}, introducing additional pyramid levels, and designing attention or alignment modules to enhance small-scale features.

Recent methods increasingly emphasize \emph{detail-aware} feature enrichment. For example, HS-FPN highlights tiny objects by generating high-frequency responses as mask weights and complements this with explicit spatial dependency modeling \cite{hs-fpn}. Context modeling (e.g., large receptive fields or multi-kernel designs) also helps disambiguate tiny objects from background clutter \cite{lsnet,fbrt-yolo}. However, many of these approaches focus on either spatial fusion or receptive-field engineering, while the \emph{mechanism of how fine details are suppressed and should be reconstructed} is often left implicit, and portability across heterogeneous detector designs is not always validated.

\subsection{Frequency-Domain Modeling for Dense Prediction}

Frequency-domain analysis offers a complementary lens to understand and manipulate representation learning. A line of work uses Fourier transforms to achieve efficient global interactions. GFNet replaces quadratic self-attention with frequency-domain filtering (FFT--filtering--IFFT), yielding log-linear complexity while maintaining global receptive fields \cite{gfnet}. Other work links common architectural operations to spectral decomposition: FcaNet interprets channel attention as a frequency-domain compression process and generalizes global pooling to multi-spectral channel attention \cite{fcanet}.

More recently, frequency-aware modules have been explored for dense prediction. FDConv observes that candidate dynamic convolution kernels often have highly similar frequency responses, and proposes constructing frequency-diverse weights by allocating parameters to disjoint Fourier indices, together with frequency-band/spatial modulation \cite{fdconv}. Frequency-aware fusion is also studied: FreqFusion explicitly introduces adaptive low-pass/high-pass filtering to improve feature consistency and boundary sharpness during upsampling and fusion \cite{freqfusion}. Wavelet-based approaches provide multi-resolution decomposition with partial spatial localization; WTConv performs convolutions in wavelet sub-bands to scale receptive fields efficiently and can be used as a drop-in layer in CNNs \cite{wtconv}.

While these spectral methods demonstrate that frequency-domain techniques can be integrated into modern architectures, existing designs are often \emph{task- or component-specific} (e.g., classification backbones, fusion-only modules, or specific convolution families), and do not provide a unified, plug-and-play operator that can be instantiated across \emph{backbone, neck, and head} \emph{and} generalize across both CNN- and Transformer-style detectors. Our work fills this gap by introducing a decomposition--reconstruction operator that preserves and re-synthesizes discriminative spectral components with minimal overhead, and systematically validating its cross-architecture generality.

%---------------------
\section{Method}

\subsection{Overall Framework}

Small object detection is inherently challenging due to the scarcity of spatial information, as small objects occupy few pixels and are dominated by high-frequency cues such as edges and fine textures. However, modern detectors suffer from three key issues: (i) backbone downsampling implicitly acts as a low-pass filter, (ii) multi-scale fusion in the neck smooths boundaries, and (iii) the head’s regression gradients are weakened when boundary information is diluted. 

To address these challenges, we propose a frequency-domain solution based on the principle of \emph{decomposing features into low and high-frequency components, selectively enhancing them, and reconstructing or injecting the enhanced signals back into the feature stream}. This solution can be captured by the \textbf{Frequency-guided Decompose--Enhance--Reconstruct (DER) operator}.

\noindent\textbf{Frequency-guided Decompose--Enhance--Reconstruct (DER) Operator.}  
Given an input feature tensor $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$, we decompose it into low- and high-frequency components using $\mathcal{D}$, enhance the components via $\mathcal{E}_{L}$ and $\mathcal{E}_{H}$, and finally reconstruct the enhanced features through $\mathcal{R}$:
\begin{equation}
\begin{aligned}
(\mathbf{X}_{L}, \mathbf{X}_{H}) &= \mathcal{D}(\mathbf{X}), \\
\mathbf{X}_{L}^{+} &= \mathcal{E}_{L}(\mathbf{X}_{L}), \quad \mathbf{X}_{H}^{+} = \mathcal{E}_{H}(\mathbf{X}_{H}), \\
\mathbf{X}^{+} &= \mathcal{R}(\mathbf{X}_{L}^{+}, \mathbf{X}_{H}^{+}).
\end{aligned}
\end{equation}
Here $\mathcal{D}$ extracts low-/high-frequency components (e.g., via wavelet transforms), $\mathcal{E}_L$ and $\mathcal{E}_H$ are lightweight enhancement functions, and $\mathcal{R}$ reconstructs the enhanced signal back into the feature stream.

\noindent\textbf{DER Instantiations Across Backbone, Neck, and Head.}  
We instantiate DER at three locations with complementary roles: WDG in the backbone preserves boundary-relevant high-frequency evidence before aggressive downsampling; LGE/LGE-W in the neck re-amplifies high-frequency residuals before multi-scale fusion to prevent detail dilution; and FDHead in the head converts high-frequency energy into a gain factor for boundary-aligned box regression. Together, they progressively preserve, enhance, and re-inject high-frequency information with minimal overhead.

\noindent\textbf{Pipeline Overview.}  
Given a baseline detector with backbone $\mathcal{B}$, neck $\mathcal{N}$, and head $\mathcal{H}$, our framework applies the DER operators as follows:
\begin{equation}
\begin{aligned}
\{\mathbf{C}_\ell\} &= \mathcal{B}(\mathbf{I}), \quad \mathbf{C}'_\ell = \begin{cases} 
\mathcal{W}(\mathbf{C}_\ell), & \ell \in \mathcal{S}_\mathcal{B}, \\
\mathbf{C}_\ell, & \text{otherwise},
\end{cases}\\
\{\mathbf{P}_\ell\} &= \mathcal{N}(\{\mathbf{C}'_\ell\}), \quad \mathbf{P}'_\ell = \mathcal{E}(\mathbf{P}_\ell), \\
\widehat{\mathbf{Y}} &= \mathcal{H}_{\mathrm{FD}}(\{\mathbf{P}'_\ell\}).
\end{aligned}
\end{equation}
Where $\mathcal{W}$, $\mathcal{E}$, and $\mathcal{H}_{\mathrm{FD}}$ represent the concrete DER instantiations for the backbone, neck, and head, respectively, and $\mathcal{S}_\mathcal{B}$ denotes the set of backbone stages where WDG is applied.


\subsection{Wavelet-Difference Gate (WDG)}

We introduce Wavelet-Difference Gate (WDG), a lightweight plug-and-play bottleneck that injects frequency-aware modulation into convolutional backbones. Given an input feature map $\mathbf{x} \in \mathbb{R}^{C\times H\times W}$, WDG first applies a $1\times 1$ projection to hidden channels $C'=\lfloor eC\rfloor$ (with expansion ratio $e$) and then performs a 2D Haar discrete wavelet transform (DWT) to separate low- and high-frequency components. For simplicity, we describe the transform for even $H,W$; in practice we align sizes by cropping/padding and restore the original resolution after reconstruction.

\noindent\textbf{Projection and wavelet decomposition.}
We first project $\mathbf{x}$ to a hidden space and decompose it into Haar subbands:
\begin{equation}
\label{eq:wdg_proj}
\begin{aligned}
\mathbf{x}' &= f_{1\times 1}(\mathbf{x}),\\
\bigl(\mathbf{x}_{LL},\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}\bigr) &= \mathrm{DWT}(\mathbf{x}').
\end{aligned}
\end{equation}
Here $\mathbf{x}_{LL}$ is the low-frequency approximation, and $\{\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}\}$ capture horizontal/vertical/diagonal high-frequency details.
This decomposition explicitly separates coarse structures from fine details, enabling targeted refinement for small objects.

For Haar DWT/IDWT, each spatial $2\times 2$ block is transformed by a $2\times 2$ Haar matrix. For each channel $c$ and location $(u,v)$, define the local block
\begin{equation}
\label{eq:haar_block}
\mathbf{X}^{(c)}_{u,v} = \begin{pmatrix}
\mathbf{x}'^{(c)}_{2u,2v} & \mathbf{x}'^{(c)}_{2u,2v+1}\\
\mathbf{x}'^{(c)}_{2u+1,2v} & \mathbf{x}'^{(c)}_{2u+1,2v+1}
\end{pmatrix}.
\end{equation}
Then Haar DWT and IDWT are given by
\begin{equation}
\label{eq:haar_dwt_idwt}
\begin{aligned}
\mathbf{S}^{(c)}_{u,v} &= \tfrac{1}{2}\,\mathbf{H}_2\,\mathbf{X}^{(c)}_{u,v}\,\mathbf{H}_2^{\top},\\
\mathbf{X}^{(c)}_{u,v} &= \tfrac{1}{2}\,\mathbf{H}_2^{\top}\,\mathbf{S}^{(c)}_{u,v}\,\mathbf{H}_2,\qquad
\mathbf{H}_2 = \begin{pmatrix} 1 & 1\\ 1 & -1 \end{pmatrix},
\end{aligned}
\end{equation}
where $\mathbf{S}^{(c)}_{u,v}=\begin{pmatrix}\mathbf{x}^{(c)}_{LL,u,v} & \mathbf{x}^{(c)}_{LH,u,v}\\ \mathbf{x}^{(c)}_{HL,u,v} & \mathbf{x}^{(c)}_{HH,u,v}\end{pmatrix}$ collects the four subbands. This matrix form is exactly equivalent to the element-wise expressions used in our implementation.

\noindent\textbf{RepCDC for low-frequency refinement.}
To enhance discriminative edges while keeping computation low, we refine the approximation subband at half resolution:
\begin{equation}
\label{eq:wdg_cdc}
\mathbf{y}_{LL} = f_{\mathrm{cdc}}(\mathbf{x}_{LL}).
\end{equation}
In our implementation, $f_{\mathrm{cdc}}$ is RepCDC followed by normalization and activation. RepCDC parameterizes a central-difference convolution by decreasing the center coefficient of a $3\times 3$ kernel with a learnable $\boldsymbol{\theta}$. Concretely, the effective kernel is
\begin{equation}
\label{eq:wdg_repcdc}
\mathbf{y}^{(o)}_{p,q} = \sum_{c}\sum_{i=-1}^{1}\sum_{j=-1}^{1} \mathbf{W}^{(o,c)}_{i,j}\,\mathbf{z}^{(c)}_{p+i,q+j} \, - \, \sum_{c} \boldsymbol{\theta}^{(o,c)}\,\mathbf{z}^{(c)}_{p,q},
\end{equation}
where $\mathbf{z}$ denotes the input to RepCDC (e.g., $\mathbf{z}=\mathbf{x}_{LL}$), and $(p,q)$ indexes spatial locations. This expression is exactly equivalent to subtracting $\boldsymbol{\theta}$ from the center coefficient of a $3\times 3$ kernel. During deployment, the resulting kernel is fused into a single standard convolution, so RepCDC incurs no extra inference branches.
Operating on $\mathbf{x}_{LL}$ reduces spatial cost by $4\times$ while strengthening edge sensitivity through the difference term.

\noindent\textbf{High-frequency gated modulation.}
We use high-frequency responses to predict a content-adaptive gate and modulate the refined low-frequency feature:
\begin{equation}
\label{eq:wdg_gate}
\begin{aligned}
\mathbf{g} &= \sigma\Bigl(f_{g}(\operatorname{Concat}(\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}))\Bigr),\\
\widetilde{\mathbf{x}}_{LL} &= \mathbf{y}_{LL} \odot (\mathbf{1}+\mathbf{g}).
\end{aligned}
\end{equation}
We use additive gating $(1+\mathbf{g})$ to emphasize informative regions without suppressing the overall magnitude of $\mathbf{y}_{LL}$. $f_g$ is a $1\times 1$ convolution followed by normalization, and $\operatorname{Concat}(\cdot)$ denotes channel-wise concatenation.
Since the gate is predicted from high-frequency subbands, it acts as a detail-aware selector that boosts regions with strong edge/texture cues.

\noindent\textbf{Reconstruction and residual output.}
Finally, we preserve the original high-frequency subbands and reconstruct the feature via inverse Haar transform:
\begin{equation}
\label{eq:wdg_recon}
\begin{aligned}
\widehat{\mathbf{x}}' &= \mathrm{IDWT}(\widetilde{\mathbf{x}}_{LL},\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}),\\
\mathbf{y} &= f_{1\times 1}^{\mathrm{out}}(\widehat{\mathbf{x}}').
\end{aligned}
\end{equation}
When input/output channels match, WDG uses a residual connection $\mathbf{y}\leftarrow\mathbf{x}+\mathbf{y}$. Since the wavelet-domain refinement operates on $H/2\times W/2$, WDG adds only a small overhead and can be inserted as a generic bottleneck into different backbone designs.
Preserving the original high-frequency subbands avoids over-smoothing and helps retain boundary sharpness after reconstruction.

\subsection{Log-Gabor Enhancer (LGE) and WTConv Variant (LGE-W)}
 
 We next improve the neck by introducing Log-Gabor Enhancer (LGE), a plug-and-play high-frequency refinement module applied to intermediate feature maps before multi-scale fusion. LGE is instantiated per feature level and is agnostic to the specific fusion topology (e.g., FPN/PAN/decoder-style aggregation).
 
 \noindent\textbf{Log-Gabor filter bank (LGF).}
Given a feature map $\mathbf{x}\in\mathbb{R}^{C\times H\times W}$, LGF applies a fixed Log-Gabor filter bank using depthwise convolutions. Let $K$ and $S$ denote the number of orientations and scales. For each channel $c$, orientation $k$, and scale $s$, we compute
\begin{equation}
\label{eq:lgn_lgf}
\mathbf{h}^{(c)}_{s,k} = \mathbf{x}^{(c)} * \mathbf{g}_{s,k},
\end{equation}
 where $\mathbf{g}_{s,k}$ is a non-learnable Log-Gabor kernel and $*$ is convolution. In our implementation, $\mathbf{g}_{s,k}$ is instantiated in the spatial domain by rotating a centered coordinate grid and applying a log-normal radial envelope with a cosine angular term:@@
 \begin{equation}
 \label{eq:lgn_loggabor}
 \begin{alignedat}{2}
 c_k &= \cos\phi_k,\qquad & s_k &= \sin\phi_k,\\
 u' &= u\,c_k + v\,s_k,\qquad & v' &= -u\,s_k + v\,c_k,\\
 r &= \sqrt{{u'}^{2}+{v'}^{2}}+\varepsilon,\qquad & \theta &= \mathrm{atan2}(v',u'),\\
 \mathbf{g}_{s,k}(u,v) &= \exp\!\left(-\frac{\log^{2}(r/\rho_s)}{2\,\log^{2}2}\right)\cos\theta.
 \end{alignedat}
 \end{equation}
 where $\phi_k = k\pi/K$ and $\rho_s$ is a fixed scale parameter. This produces a set of directional subband responses that explicitly emphasize edges and fine textures while introducing no additional learnable filter parameters.
 
 \noindent\textbf{Learnable aggregation and residual enhancement (LGE).}
LGE aggregates the subbands with learnable orientation/scale importance. Let $\boldsymbol{\alpha}\in\mathbb{R}^{S}$ and $\boldsymbol{\beta}\in\mathbb{R}^{K}$ be learnable logits; we obtain normalized weights by softmax and compute the high-frequency summary
\begin{equation}
\label{eq:lgn_agg}
\mathbf{h}^{(c)} = \sum_{s=1}^{S}\sum_{k=1}^{K} \operatorname{softmax}(\boldsymbol{\alpha})_{s}\,\operatorname{softmax}(\boldsymbol{\beta})_{k}\,\mathbf{h}^{(c)}_{s,k}.
\end{equation}
We further apply a learnable global scale $\gamma$ (implemented as a sigmoid-gated parameter) and a local mixing operator $f_{\mathrm{mix}}$:
\begin{equation}
\label{eq:lgn_out}
\mathbf{y} = \mathbf{x}_{\mathrm{skip}} + f_{\mathrm{mix}}\bigl(\sigma(\gamma)\,\mathbf{h}\bigr).
\end{equation}
Here $\mathbf{x}_{\mathrm{skip}}$ is either the identity mapping (when channels match) or a $1\times 1$ projection. In our implementation, $f_{\mathrm{mix}}$ is a $3\times 3$ convolution (depthwise when $C$ is preserved), so LGE adds only local mixing on top of fixed spectral decomposition while keeping a residual pathway.

\noindent\textbf{Wavelet variant (LGE-W).}
LGE-W follows Eq.~\eqref{eq:lgn_lgf}--\eqref{eq:lgn_out} but replaces $f_{\mathrm{mix}}$ with a wavelet-transform convolution (WTConv) when $C$ is preserved. Using a fixed wavelet (Haar/\texttt{db1}), WTConv performs subband mixing in the wavelet domain and adds a lightweight depthwise branch:@@
 \begin{equation}
 \label{eq:lgew_wtconv}
 \mathrm{WTConv}(\mathbf{z}) = \mathcal{S}_0\,\mathcal{D}_0(\mathbf{z}) + \mathrm{IDWT}\!\left(\mathcal{S}\,\mathcal{D}_{4}\bigl(\mathrm{DWT}(\mathbf{z})\bigr)\right),
 \end{equation}
 where $\mathcal{D}_0$ is a depthwise convolution in the spatial domain and $\mathcal{D}_{4}$ denotes grouped depthwise convolutions applied over the four wavelet subbands.
@@
 \subsection{Frequency-Driven Head (FDHead)}
 
 We finally introduce Frequency-Driven Head (FDHead), a frequency-aware detection head that improves small-object localization by injecting a boundary-sensitive prior into dense regression while preserving the standard anchor-free interface. FDHead is instantiated over multi-scale feature maps $\{\mathbf{x}_i\}_{i=1}^{N}$ (e.g., $P2$--$P5$) and shares most head parameters across levels to reduce capacity fragmentation.
 
 \noindent\textbf{Shared prediction tower.}
 For each level $i$, FDHead first aligns channels to a hidden width $C_h$ (Conv+GroupNorm) and then applies a shared refinement stack (DEConv + depthwise--pointwise mixing). The DEConv block aggregates multiple directional-difference operators (center/adjacent/horizontal/vertical) and a standard kernel; at inference it can be written as a single convolution with merged weights:
 \begin{equation}
 \label{eq:fdhead_deconv}
 \mathrm{DEConv}(\mathbf{u}) = \varphi\Bigl(\bigl(\sum_{m} \mathbf{K}_{m}\bigr) * \mathbf{u} + \sum_{m}\mathbf{b}_{m}\Bigr),
 \end{equation}
 where $m$ indexes the directional branches and $\varphi(\cdot)$ denotes normalization and activation. This biases the shared tower toward contour-aware features that are beneficial for boundary-aligned regression.
 \begin{equation}
 \label{eq:fdhead_shared}
 \mathbf{f}_i = \mathcal{T}(\mathbf{x}_i),\qquad \mathcal{T}=\mathcal{T}_{\mathrm{share}}\circ \mathcal{T}_{1\times 1}.
 \end{equation}
 
 \noindent\textbf{P2 high-frequency gate.}
 Since the finest level ($P2$) carries the most precise spatial details, FDHead applies a lightweight wavelet gate only on $i=1$ (corresponding to $P2$). Let $C_f$ be the gated channel width (set as a fraction of $C_h$); we split channels $\mathbf{f}_1=[\mathbf{f}_a,\mathbf{f}_b]$ with $\mathbf{f}_a\in\mathbb{R}^{C_f\times H\times W}$. Using a fixed Haar transform, we estimate high-frequency energy as a softmax-weighted mixture of subband magnitudes and convert it to a channel-wise gain:
 \begin{equation}
 \label{eq:fdhead_hfgate}
 \begin{aligned}
 (\mathbf{f}_{LL},\mathbf{f}_{LH},\mathbf{f}_{HL},\mathbf{f}_{HH}) &= \mathrm{DWT}(\mathbf{f}_a),\\
 \mathbf{w} &= \operatorname{softmax}(\boldsymbol{\omega}),\\
 \mathbf{h} &= w_{LH}\,|\mathbf{f}_{LH}| + w_{HL}\,|\mathbf{f}_{HL}| + w_{HH}\,|\mathbf{f}_{HH}|,\\
 \mathbf{g} &= \mathrm{Gate}\bigl(\operatorname{AvgPool}(\mathbf{h})\bigr),\\
 \widetilde{\mathbf{f}}_a &= \mathbf{f}_a\odot\bigl(1+\alpha\,\mathbf{g}\bigr).
 \end{aligned}
 \end{equation}
 Here $\boldsymbol{\omega}$ are learnable logits over $\{LH,HL,HH\}$ and $\alpha$ controls the gate strength. $\mathrm{Gate}(\cdot)$ is a squeeze-excitation style channel MLP (two $1\times 1$ convs with sigmoid output) driven by pooled high-frequency energy. We then form $\widetilde{\mathbf{f}}_1=[\widetilde{\mathbf{f}}_a,\mathbf{f}_b]$ and apply it only to the box branch: high-frequency energy is a direct proxy for boundary sharpness and thus improves offset estimation, while leaving the classification stream unchanged avoids over-fitting to textures and background clutter.
 For the remaining levels $i>1$, we set $\widetilde{\mathbf{f}}_i=\mathbf{f}_i$.
 
 \noindent\textbf{Box/class prediction and decoding.}
 FDHead predicts per-location class logits and distributional box offsets (DFL) as
 \begin{equation}
 \label{eq:fdhead_pred}
 \mathbf{b}_i = \mathrm{Scale}_i\bigl(\mathcal{H}_{\mathrm{box}}(\widetilde{\mathbf{f}}_i)\bigr),\qquad
 \mathbf{p}_i = \mathcal{H}_{\mathrm{cls}}(\mathbf{f}_i),
 \end{equation}
 and decodes boxes by $\widehat{\mathbf{B}}=\mathrm{dist2bbox}(\mathrm{DFL}(\mathbf{b}),\mathbf{A})\cdot\mathbf{s}$ with anchors $\mathbf{A}$ and strides $\mathbf{s}$. This design targets small objects by frequency-gating only the finest level while keeping the remaining head computation shared and lightweight.

 %---------------------
 \section{Experiment}
 \subsection{Datasets and Metrics}
We evaluate our framework on four benchmarks to demonstrate its robustness and cross-domain generalization: VisDrone2019 \cite{du2019visdrone}, TinyPerson \cite{yu2020scale}, UAVDT \cite{du2018unmanned}, and DOTA v1 \cite{xia2018dota}. \textbf{VisDrone2019} is our primary benchmark and is particularly challenging due to dense small objects and severe scale variation, where most targets are smaller than $50 \times 50$ pixels.

We report both accuracy and efficiency, including mAP$_{50}$, the number of parameters, GFLOPs, model size, and FPS.

\subsection{Configuration}
The experimental configuration is detailed in Table \ref{tab:config}.

% --- 表格部分 (保持不变) ---
\begin{table}[ht]
    \centering
    \caption{\textbf{Configuration of Training and Testing Experiment Environments.} Detailed hardware and software configuration used for all experiments in this study.}
    \label{tab:config}
    \vspace{0.2cm} 
    
    \begin{tabular}{ll} 
        \toprule
        \textbf{Environment} & \textbf{Parameter} \\
        \midrule
        CPU              & Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz \\
        GPU              & NVIDIA A100-PCIE-40GB \\
        VRAM             & 40 GB \\
        RAM              & 46 GB \\
        Operating System & Rocky Linux 8.5 (Green Obsidian) \\
        Language         & Python 3.10.14 \\
        Frame            & PyTorch 2.1.0 \\
        CUDA Version     & 12.6 \\
        \bottomrule
    \end{tabular}
\end{table}
%这个表格需要调整大小/格式
For YOLO-style architectures, models are trained for 300 epochs with an input resolution of $640 \times 640$ and batch size 16, using SGD optimization. Unless otherwise specified, Mosaic augmentation is enabled throughout training; we use 4 dataloader workers and disable AMP.
%其他的架构还没写


%---------------------
\section{Main Results}

\subsection{Ablation Study on YOLO-style architectures}


%------消融表格------
\begin{table}[t]
\centering
Top three performances per metric are highlighted from
\colorbox[HTML]{FFB366}{Dark} (best) to \colorbox[HTML]{FFE5CC}{Light} (third).
\setlength\tabcolsep{2pt}
\renewcommand{\arraystretch}{0.8}
\fontsize{6pt}{7.5pt}\selectfont
\resizebox{1\columnwidth}{!}{
\begin{tabular}{@{}cccc *{6}{c} *{4}{c}@{}}
\toprule
\multirow{3}{*}{\tiny\textbf{P2 head}}
& \multirow{3}{*}{\tiny\textbf{WDG}}
& \multirow{3}{*}{\tiny\textbf{LGE}}
& \multirow{3}{*}{\tiny\textbf{FDHead}}
& \multicolumn{6}{c}{\tiny\textbf{Performance}}
& \multicolumn{4}{c}{\tiny\textbf{Efficiency}} \\
\cmidrule(lr){5-10}\cmidrule(lr){11-14}
& & & & \multicolumn{3}{c}{\tiny\textbf{Val}} & \multicolumn{3}{c}{\tiny\textbf{Test}} & & & & \\
\cmidrule(lr){5-7}\cmidrule(lr){8-10}
\rowcolor{blue!5}
& & & &
\tiny\textbf{P} & \tiny\textbf{R} & \tiny\textbf{mAP50}
& \tiny\textbf{P} & \tiny\textbf{R} & \tiny\textbf{mAP50}
& \tiny\textbf{Params/M} & \tiny\textbf{GFLOPs} & \tiny\textbf{FPS} & \tiny\textbf{Model size/MB} \\
\midrule
$\times$ & $\times$ & $\times$ & $\times$
& 0.496 & 0.377 & \mapcell{0.384}{} & 0.421 & 0.337 & \mapcell{0.311}{} & 9.4 & 21.6 & -- & 18.3 \\
\checkmark & $\times$ & $\times$ & $\times$
& 0.516 & 0.408 & \mapcell{0.423}{} & 0.453 & 0.354 & \mapcell{0.337}{} & 6.4 & 24.7 & -- & 12.5 \\
\checkmark & \checkmark & $\times$ & $\times$
& 0.543 & 0.420 & \mapcell{0.438}{0.015} & 0.455 & 0.371 & \mapcell{0.354}{0.017} & 6.6 & 23.5 & -- & 13.2 \\
\checkmark & $\times$ & \checkmark & $\times$
& 0.547 & 0.412 & \mapcell{0.436}{0.013} & 0.465 & 0.373 & \mapcell{0.355}{0.018} & 6.5 & 25.4 & -- & 12.8 \\
\checkmark & $\times$ & $\times$ & \checkmark
& 0.551 & 0.435 & \mapcell{0.454}{0.031} & 0.478 & 0.375 & \mapcell{0.364}{0.027} & 3.8 & 30.5 & -- & 8.4 \\
\checkmark & \checkmark & \checkmark & $\times$
& 0.547 & 0.419 & \mapcell{0.445}{0.022} & 0.466 & 0.376 & \mapcell{0.360}{0.023} & 3.9 & 25.5 & -- & 8.2 \\
\checkmark & \checkmark & $\times$ & \checkmark
& 0.559 & 0.435 & \mapcell{0.464}{0.041} & 0.487 & 0.383 & \mapcell{0.370}{0.033} & 3.9 & 28.5 & -- & 8.6 \\
\checkmark & $\times$ & \checkmark & \checkmark
& 0.549 & 0.437 & \mapcell{0.457}{0.034} & 0.470 & 0.378 & \mapcell{0.365}{0.028} & 3.8 & 28.3 & -- & 8.4 \\
\checkmark & \checkmark & \checkmark & \checkmark
& 0.552 & 0.429 & \mapcell{0.458}{0.035} & 0.484 & 0.381 & \mapcell{0.370}{0.033} & 3.8 & 26.3 & -- & 8.6 \\
\checkmark & \multicolumn{3}{c}{Lite}
& 0.503 & 0.382 & \mapcell{0.398}{} & 0.420 & 0.344 & \mapcell{0.316}{} & 1.3 & 13.3 & -- & 3.4 \\
\bottomrule
\end{tabular}}
\caption{Ablation study on YOLO-style architectures.}
\label{tab:ablation}
\vspace{-1mm}
\end{table}


%-------------------

\subsection{Across-architecture Study}

\subsection{Comparison with State-of-the-art}

%---------------------
\section{Analyses and Discussion}




%---------------------
\section{Conclusion}










%---------------------
\bibliography{example_paper}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$
pages long. For the final version, one more page can be added. If you want, you
can use an appendix like this one.

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you
prefer a one-column appendix, or can be removed if you prefer a two-column
appendix.  Apart from this possible change, the style (font size, spacing,
margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
