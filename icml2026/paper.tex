%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% For preprint, use
% \usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2026}

\begin{document}

\twocolumn[
  \icmltitle{From Spatial to Spectral: An Efficient, Frequency-Guided Representation Learner for Small Object Detection}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Firstname1 Lastname1}{equal,yyy}
    \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
    \icmlauthor{Firstname3 Lastname3}{comp}
    \icmlauthor{Firstname4 Lastname4}{sch}
    \icmlauthor{Firstname5 Lastname5}{yyy}
    \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    \icmlauthor{Firstname8 Lastname8}{sch}
    \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
  \end{icmlauthorlist}

  \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
  \icmlaffiliation{comp}{Company Name, Location, Country}
  \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

  \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
  \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{Machine Learning, ICML}

  \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.
% If you have no special notice, KEEP empty braces:
\printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
% \printAffiliationsAndNotice{\icmlEqualContribution}



%---------------------
\begin{abstract}
  This document provides a basic paper template and submission guidelines.
  Abstracts must be a single paragraph, ideally between 4--6 sentences long.
  Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

%---------------------
\section{Introduction}



%---------------------
\section{Related Work}

We review prior work from three angles that are most relevant to our goal: (i) efficient detector architectures, (ii) small object detection strategies, and (iii) frequency-domain modeling for dense prediction.

\subsection{Efficient Detector Architectures}

Real-time detection has been driven by architectural efficiency in backbones, feature pyramids, and heads. One-stage YOLO-style detectors optimize the accuracy--latency trade-off through carefully designed blocks and multi-scale prediction, with recent variants continuing to improve both speed and accuracy \cite{yolov10,yolov11}. Lightweight enhancements for challenging regimes (e.g., cluttered scenes) often rely on stronger feature aggregation or multi-kernel perception to increase representational diversity while keeping inference efficient \cite{fbrt-yolo}. 

In parallel, Transformer-based detectors seek end-to-end set prediction by removing hand-crafted components such as anchors \cite{detr}. Subsequent work improves the practicality of DETR-like models via more efficient attention and training strategies, enabling competitive performance under constrained budgets \cite{rt-detr,dino}. Despite these advances, both CNN- and Transformer-based detectors still face a common tension for tiny/small objects: improving fine-detail sensitivity typically increases computation, memory, or architectural intrusion, making it difficult to deploy a uniformly effective solution across detector families.

\subsection{Small Object Detection}

Small objects are inherently information-limited: they occupy few pixels, induce weak feature responses, and are easily suppressed by downsampling and coarse fusion. Early two-stage and one-stage frameworks (e.g., Faster R-CNN and SSD) already revealed the difficulty of preserving small-object cues under feature hierarchy and stride growth \cite{fasterrcnn,ssd}. A large body of work improves small-object performance by strengthening multi-scale feature fusion (e.g., FPN and its variants) \cite{fpn}, introducing additional pyramid levels, and designing attention or alignment modules to enhance small-scale features.

Recent methods increasingly emphasize \emph{detail-aware} feature enrichment. For example, HS-FPN highlights tiny objects by generating high-frequency responses as mask weights and complements this with explicit spatial dependency modeling \cite{hs-fpn}. Context modeling (e.g., large receptive fields or multi-kernel designs) also helps disambiguate tiny objects from background clutter \cite{lsnet,fbrt-yolo}. However, many of these approaches focus on either spatial fusion or receptive-field engineering, while the \emph{mechanism of how fine details are suppressed and should be reconstructed} is often left implicit, and portability across heterogeneous detector designs is not always validated.

\subsection{Frequency-Domain Modeling for Dense Prediction}

Frequency-domain analysis offers a complementary lens to understand and manipulate representation learning. A line of work uses Fourier transforms to achieve efficient global interactions. GFNet replaces quadratic self-attention with frequency-domain filtering (FFT--filtering--IFFT), yielding log-linear complexity while maintaining global receptive fields \cite{gfnet}. Other work links common architectural operations to spectral decomposition: FcaNet interprets channel attention as a frequency-domain compression process and generalizes global pooling to multi-spectral channel attention \cite{fcanet}.

More recently, frequency-aware modules have been explored for dense prediction. FDConv observes that candidate dynamic convolution kernels often have highly similar frequency responses, and proposes constructing frequency-diverse weights by allocating parameters to disjoint Fourier indices, together with frequency-band/spatial modulation \cite{fdconv}. Frequency-aware fusion is also studied: FreqFusion explicitly introduces adaptive low-pass/high-pass filtering to improve feature consistency and boundary sharpness during upsampling and fusion \cite{freqfusion}. Wavelet-based approaches provide multi-resolution decomposition with partial spatial localization; WTConv performs convolutions in wavelet sub-bands to scale receptive fields efficiently and can be used as a drop-in layer in CNNs \cite{wtconv}.

While these spectral methods demonstrate that frequency-domain techniques can be integrated into modern architectures, existing designs are often \emph{task- or component-specific} (e.g., classification backbones, fusion-only modules, or specific convolution families), and do not provide a unified, plug-and-play operator that can be instantiated across \emph{backbone, neck, and head} \emph{and} generalize across both CNN- and Transformer-style detectors. Our work fills this gap by introducing a decomposition--reconstruction operator that preserves and re-synthesizes discriminative spectral components with minimal overhead, and systematically validating its cross-architecture generality.

%---------------------
\section{Method}

\subsection{Overall Framework}

Small objects occupy few pixels and are therefore dominated by high spatial frequencies (edges and fine textures). However, in modern detectors these components are progressively attenuated: backbone downsampling behaves as an implicit low-pass operator, neck fusion/upsampling further smooths boundaries, and the head's regression gradients weaken once boundary evidence is diluted. Our key observation is that these three failure modes can be addressed by a single frequency-domain principle: \emph{decompose features into low/high-frequency components, selectively enhance them, and reconstruct (or inject) the enhanced signal}.

\noindent\textbf{Unified decompose--enhance--reconstruct (DER) operator.}
Given a feature tensor $\mathbf{X}\in\mathbb{R}^{C\times H\times W}$, we define
\begin{equation}
\label{eq:overall_hf_inject}
\begin{aligned}
(\mathbf{X}_{L},\mathbf{X}_{H}) &= \mathcal{D}(\mathbf{X}),\\
\mathbf{X}_{L}^{+} &= \mathcal{E}_{L}(\mathbf{X}_{L}),\qquad \mathbf{X}_{H}^{+} = \mathcal{E}_{H}(\mathbf{X}_{H}),\\
\mathbf{X}^{+} &= \mathcal{R}(\mathbf{X}_{L}^{+},\mathbf{X}_{H}^{+}),
\end{aligned}
\end{equation}
where $\mathcal{D}$ extracts low-/high-frequency evidence (wavelet subbands or oriented band-pass responses), $\mathcal{E}_{L}$ and $\mathcal{E}_{H}$ are lightweight enhancement functions, and $\mathcal{R}$ reconstructs or injects the enhanced signal back to the spatial feature stream.

This DER operator is instantiated at three locations with complementary roles along the detector pipeline and forms a coherent frequency flow: WDG performs DER early so that boundary-relevant $\mathbf{X}_{H}$ is not prematurely suppressed and can be propagated to the neck; LGE/LGE-W applies DER again \emph{before} multi-scale fusion so that the enhanced high-frequency residual survives aggregation; FDHead finally converts the resulting high-frequency energy into a regression gain, making the localization head sensitive to boundary evidence that has been preserved and re-amplified upstream.

\noindent\textbf{DER instantiations across backbone/neck/head.}
In our implementation, the three modules correspond to Eq.~\eqref{eq:overall_hf_inject} with different choices of $\mathcal{D}$, $\mathcal{E}_{L/H}$, and $\mathcal{R}$:
\begin{equation}
\label{eq:der_instances}
\begin{aligned}
\text{WDG:}\ & \mathcal{D}=\mathrm{DWT},\ \mathcal{E}_{L}=f_{\mathrm{cdc}},\ \mathcal{E}_{H}=\mathrm{Id},\ \mathcal{R}=\mathrm{IDWT};\\
\text{LGE/LGE-W:}\ & \mathcal{D}=\mathrm{LGF\ (or\ DWT)},\ \mathcal{E}_{H}:\eqref{eq:lgn_agg},\ \mathcal{R}:\eqref{eq:lgn_out},\ \text{(or}\ \eqref{eq:lgew_wtconv}\text{)};\\
\text{FDHead:}\ & \mathcal{D}=\mathrm{DWT},\ \mathcal{R}:\eqref{eq:fdhead_hfgate}\rightarrow\eqref{eq:fdhead_pred}.
\end{aligned}
\end{equation}
This shared DER view clarifies why the three changes are not independent: they progressively preserve, re-inject, and exploit the same high-frequency evidence across backbone--neck--head.

Formally, given a baseline detector with backbone $\mathcal{B}$, neck $\mathcal{N}$, and head $\mathcal{H}$, we instantiate the three operators as
\begin{equation}
\label{eq:overall_pipeline}
\begin{aligned}
\{\mathbf{C}_\ell\} &= \mathcal{B}(\mathbf{I}),\qquad \mathbf{C}'_\ell = \begin{cases}
\mathcal{W}(\mathbf{C}_\ell), & \ell\in\mathcal{S}_\mathcal{B},\\
\mathbf{C}_\ell, & \text{otherwise},
\end{cases}\\
\{\mathbf{P}_\ell\} &= \mathcal{N}(\{\mathbf{C}'_\ell\}),\qquad \mathbf{P}'_\ell = \mathcal{E}(\mathbf{P}_\ell),\\
\widehat{\mathbf{Y}} &= \mathcal{H}_{\mathrm{FD}}(\{\mathbf{P}'_\ell\}).
\end{aligned}
\end{equation}
Here $\mathcal{W}$, $\mathcal{E}$, and $\mathcal{H}_{\mathrm{FD}}$ are the concrete DER instantiations for backbone, neck, and head, respectively. $\mathcal{S}_\mathcal{B}$ denotes the set of backbone stages where WDG is inserted.

\subsection{Wavelet-Difference Gate (WDG)}

We introduce Wavelet-Difference Gate (WDG), a lightweight plug-and-play bottleneck that injects frequency-aware modulation into convolutional backbones. Given an input feature map $\mathbf{x} \in \mathbb{R}^{C\times H\times W}$, WDG first applies a $1\times 1$ projection to hidden channels $C'=\lfloor eC\rfloor$ (with expansion ratio $e$) and then performs a 2D Haar discrete wavelet transform (DWT) to separate low- and high-frequency components. For simplicity, we describe the transform for even $H,W$; in practice we align sizes by cropping/padding and restore the original resolution after reconstruction.

\noindent\textbf{Projection and wavelet decomposition.}
We first project $\mathbf{x}$ to a hidden space and decompose it into Haar subbands:
\begin{equation}
\label{eq:wdg_proj}
\begin{aligned}
\mathbf{x}' &= f_{1\times 1}(\mathbf{x}),\\
\bigl(\mathbf{x}_{LL},\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}\bigr) &= \mathrm{DWT}(\mathbf{x}').
\end{aligned}
\end{equation}
Here $\mathbf{x}_{LL}$ is the low-frequency approximation, and $\{\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}\}$ capture horizontal/vertical/diagonal high-frequency details.
This decomposition explicitly separates coarse structures from fine details, enabling targeted refinement for small objects.

For Haar DWT/IDWT, each spatial $2\times 2$ block is transformed by a $2\times 2$ Haar matrix. For each channel $c$ and location $(u,v)$, define the local block
\begin{equation}
\label{eq:haar_block}
\mathbf{X}^{(c)}_{u,v} = \begin{pmatrix}
\mathbf{x}'^{(c)}_{2u,2v} & \mathbf{x}'^{(c)}_{2u,2v+1}\\
\mathbf{x}'^{(c)}_{2u+1,2v} & \mathbf{x}'^{(c)}_{2u+1,2v+1}
\end{pmatrix}.
\end{equation}
Then Haar DWT and IDWT are given by
\begin{equation}
\label{eq:haar_dwt_idwt}
\begin{aligned}
\mathbf{S}^{(c)}_{u,v} &= \tfrac{1}{2}\,\mathbf{H}_2\,\mathbf{X}^{(c)}_{u,v}\,\mathbf{H}_2^{\top},\\
\mathbf{X}^{(c)}_{u,v} &= \tfrac{1}{2}\,\mathbf{H}_2^{\top}\,\mathbf{S}^{(c)}_{u,v}\,\mathbf{H}_2,\qquad
\mathbf{H}_2 = \begin{pmatrix} 1 & 1\\ 1 & -1 \end{pmatrix},
\end{aligned}
\end{equation}
where $\mathbf{S}^{(c)}_{u,v}=\begin{pmatrix}\mathbf{x}^{(c)}_{LL,u,v} & \mathbf{x}^{(c)}_{LH,u,v}\\ \mathbf{x}^{(c)}_{HL,u,v} & \mathbf{x}^{(c)}_{HH,u,v}\end{pmatrix}$ collects the four subbands. This matrix form is exactly equivalent to the element-wise expressions used in our implementation.

\noindent\textbf{RepCDC for low-frequency refinement.}
To enhance discriminative edges while keeping computation low, we refine the approximation subband at half resolution:
\begin{equation}
\label{eq:wdg_cdc}
\mathbf{y}_{LL} = f_{\mathrm{cdc}}(\mathbf{x}_{LL}).
\end{equation}
In our implementation, $f_{\mathrm{cdc}}$ is RepCDC followed by normalization and activation. RepCDC parameterizes a central-difference convolution by decreasing the center coefficient of a $3\times 3$ kernel with a learnable $\boldsymbol{\theta}$. Concretely, the effective kernel is
\begin{equation}
\label{eq:wdg_repcdc}
\mathbf{y}^{(o)}_{p,q} = \sum_{c}\sum_{i=-1}^{1}\sum_{j=-1}^{1} \mathbf{W}^{(o,c)}_{i,j}\,\mathbf{z}^{(c)}_{p+i,q+j} \, - \, \sum_{c} \boldsymbol{\theta}^{(o,c)}\,\mathbf{z}^{(c)}_{p,q},
\end{equation}
where $\mathbf{z}$ denotes the input to RepCDC (e.g., $\mathbf{z}=\mathbf{x}_{LL}$), and $(p,q)$ indexes spatial locations. This expression is exactly equivalent to subtracting $\boldsymbol{\theta}$ from the center coefficient of a $3\times 3$ kernel. During deployment, the resulting kernel is fused into a single standard convolution, so RepCDC incurs no extra inference branches.
Operating on $\mathbf{x}_{LL}$ reduces spatial cost by $4\times$ while strengthening edge sensitivity through the difference term.

\noindent\textbf{High-frequency gated modulation.}
We use high-frequency responses to predict a content-adaptive gate and modulate the refined low-frequency feature:
\begin{equation}
\label{eq:wdg_gate}
\begin{aligned}
\mathbf{g} &= \sigma\Bigl(f_{g}(\operatorname{Concat}(\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}))\Bigr),\\
\widetilde{\mathbf{x}}_{LL} &= \mathbf{y}_{LL} \odot (\mathbf{1}+\mathbf{g}).
\end{aligned}
\end{equation}
We use additive gating $(1+\mathbf{g})$ to emphasize informative regions without suppressing the overall magnitude of $\mathbf{y}_{LL}$. $f_g$ is a $1\times 1$ convolution followed by normalization, and $\operatorname{Concat}(\cdot)$ denotes channel-wise concatenation.
Since the gate is predicted from high-frequency subbands, it acts as a detail-aware selector that boosts regions with strong edge/texture cues.

\noindent\textbf{Reconstruction and residual output.}
Finally, we preserve the original high-frequency subbands and reconstruct the feature via inverse Haar transform:
\begin{equation}
\label{eq:wdg_recon}
\begin{aligned}
\widehat{\mathbf{x}}' &= \mathrm{IDWT}(\widetilde{\mathbf{x}}_{LL},\mathbf{x}_{LH},\mathbf{x}_{HL},\mathbf{x}_{HH}),\\
\mathbf{y} &= f_{1\times 1}^{\mathrm{out}}(\widehat{\mathbf{x}}').
\end{aligned}
\end{equation}
When input/output channels match, WDG uses a residual connection $\mathbf{y}\leftarrow\mathbf{x}+\mathbf{y}$. Since the wavelet-domain refinement operates on $H/2\times W/2$, WDG adds only a small overhead and can be inserted as a generic bottleneck into different backbone designs.
Preserving the original high-frequency subbands avoids over-smoothing and helps retain boundary sharpness after reconstruction.

\subsection{Log-Gabor Enhancer (LGE) and WTConv Variant (LGE-W)}
 
 We next improve the neck by introducing Log-Gabor Enhancer (LGE), a plug-and-play high-frequency refinement module applied to intermediate feature maps before multi-scale fusion. LGE is instantiated per feature level and is agnostic to the specific fusion topology (e.g., FPN/PAN/decoder-style aggregation).
 
 \noindent\textbf{Log-Gabor filter bank (LGF).}
Given a feature map $\mathbf{x}\in\mathbb{R}^{C\times H\times W}$, LGF applies a fixed Log-Gabor filter bank using depthwise convolutions. Let $K$ and $S$ denote the number of orientations and scales. For each channel $c$, orientation $k$, and scale $s$, we compute
\begin{equation}
\label{eq:lgn_lgf}
\mathbf{h}^{(c)}_{s,k} = \mathbf{x}^{(c)} * \mathbf{g}_{s,k},
\end{equation}
 where $\mathbf{g}_{s,k}$ is a non-learnable Log-Gabor kernel and $*$ is convolution. In our implementation, $\mathbf{g}_{s,k}$ is instantiated in the spatial domain by rotating a centered coordinate grid and applying a log-normal radial envelope with a cosine angular term:@@
 \begin{equation}
 \label{eq:lgn_loggabor}
 \begin{alignedat}{2}
 c_k &= \cos\phi_k,\qquad & s_k &= \sin\phi_k,\\
 u' &= u\,c_k + v\,s_k,\qquad & v' &= -u\,s_k + v\,c_k,\\
 r &= \sqrt{{u'}^{2}+{v'}^{2}}+\varepsilon,\qquad & \theta &= \mathrm{atan2}(v',u'),\\
 \mathbf{g}_{s,k}(u,v) &= \exp\!\left(-\frac{\log^{2}(r/\rho_s)}{2\,\log^{2}2}\right)\cos\theta.
 \end{alignedat}
 \end{equation}
 where $\phi_k = k\pi/K$ and $\rho_s$ is a fixed scale parameter. This produces a set of directional subband responses that explicitly emphasize edges and fine textures while introducing no additional learnable filter parameters.
 
 \noindent\textbf{Learnable aggregation and residual enhancement (LGE).}
LGE aggregates the subbands with learnable orientation/scale importance. Let $\boldsymbol{\alpha}\in\mathbb{R}^{S}$ and $\boldsymbol{\beta}\in\mathbb{R}^{K}$ be learnable logits; we obtain normalized weights by softmax and compute the high-frequency summary
\begin{equation}
\label{eq:lgn_agg}
\mathbf{h}^{(c)} = \sum_{s=1}^{S}\sum_{k=1}^{K} \operatorname{softmax}(\boldsymbol{\alpha})_{s}\,\operatorname{softmax}(\boldsymbol{\beta})_{k}\,\mathbf{h}^{(c)}_{s,k}.
\end{equation}
We further apply a learnable global scale $\gamma$ (implemented as a sigmoid-gated parameter) and a local mixing operator $f_{\mathrm{mix}}$:
\begin{equation}
\label{eq:lgn_out}
\mathbf{y} = \mathbf{x}_{\mathrm{skip}} + f_{\mathrm{mix}}\bigl(\sigma(\gamma)\,\mathbf{h}\bigr).
\end{equation}
Here $\mathbf{x}_{\mathrm{skip}}$ is either the identity mapping (when channels match) or a $1\times 1$ projection. In our implementation, $f_{\mathrm{mix}}$ is a $3\times 3$ convolution (depthwise when $C$ is preserved), so LGE adds only local mixing on top of fixed spectral decomposition while keeping a residual pathway.

\noindent\textbf{Wavelet variant (LGE-W).}
LGE-W follows Eq.~\eqref{eq:lgn_lgf}--\eqref{eq:lgn_out} but replaces $f_{\mathrm{mix}}$ with a wavelet-transform convolution (WTConv) when $C$ is preserved. Using a fixed wavelet (Haar/\texttt{db1}), WTConv performs subband mixing in the wavelet domain and adds a lightweight depthwise branch:@@
 \begin{equation}
 \label{eq:lgew_wtconv}
 \mathrm{WTConv}(\mathbf{z}) = \mathcal{S}_0\,\mathcal{D}_0(\mathbf{z}) + \mathrm{IDWT}\!\left(\mathcal{S}\,\mathcal{D}_{4}\bigl(\mathrm{DWT}(\mathbf{z})\bigr)\right),
 \end{equation}
 where $\mathcal{D}_0$ is a depthwise convolution in the spatial domain and $\mathcal{D}_{4}$ denotes grouped depthwise convolutions applied over the four wavelet subbands.
@@
 \subsection{Frequency-Driven Head (FDHead)}
 
 We finally introduce Frequency-Driven Head (FDHead), a frequency-aware detection head that improves small-object localization by injecting a boundary-sensitive prior into dense regression while preserving the standard anchor-free interface. FDHead is instantiated over multi-scale feature maps $\{\mathbf{x}_i\}_{i=1}^{N}$ (e.g., $P2$--$P5$) and shares most head parameters across levels to reduce capacity fragmentation.
 
 \noindent\textbf{Shared prediction tower.}
 For each level $i$, FDHead first aligns channels to a hidden width $C_h$ (Conv+GroupNorm) and then applies a shared refinement stack (DEConv + depthwise--pointwise mixing). The DEConv block aggregates multiple directional-difference operators (center/adjacent/horizontal/vertical) and a standard kernel; at inference it can be written as a single convolution with merged weights:
 \begin{equation}
 \label{eq:fdhead_deconv}
 \mathrm{DEConv}(\mathbf{u}) = \varphi\Bigl(\bigl(\sum_{m} \mathbf{K}_{m}\bigr) * \mathbf{u} + \sum_{m}\mathbf{b}_{m}\Bigr),
 \end{equation}
 where $m$ indexes the directional branches and $\varphi(\cdot)$ denotes normalization and activation. This biases the shared tower toward contour-aware features that are beneficial for boundary-aligned regression.
 \begin{equation}
 \label{eq:fdhead_shared}
 \mathbf{f}_i = \mathcal{T}(\mathbf{x}_i),\qquad \mathcal{T}=\mathcal{T}_{\mathrm{share}}\circ \mathcal{T}_{1\times 1}.
 \end{equation}
 
 \noindent\textbf{P2 high-frequency gate.}
 Since the finest level ($P2$) carries the most precise spatial details, FDHead applies a lightweight wavelet gate only on $i=1$ (corresponding to $P2$). Let $C_f$ be the gated channel width (set as a fraction of $C_h$); we split channels $\mathbf{f}_1=[\mathbf{f}_a,\mathbf{f}_b]$ with $\mathbf{f}_a\in\mathbb{R}^{C_f\times H\times W}$. Using a fixed Haar transform, we estimate high-frequency energy as a softmax-weighted mixture of subband magnitudes and convert it to a channel-wise gain:
 \begin{equation}
 \label{eq:fdhead_hfgate}
 \begin{aligned}
 (\mathbf{f}_{LL},\mathbf{f}_{LH},\mathbf{f}_{HL},\mathbf{f}_{HH}) &= \mathrm{DWT}(\mathbf{f}_a),\\
 \mathbf{w} &= \operatorname{softmax}(\boldsymbol{\omega}),\\
 \mathbf{h} &= w_{LH}\,|\mathbf{f}_{LH}| + w_{HL}\,|\mathbf{f}_{HL}| + w_{HH}\,|\mathbf{f}_{HH}|,\\
 \mathbf{g} &= \mathrm{Gate}\bigl(\operatorname{AvgPool}(\mathbf{h})\bigr),\\
 \widetilde{\mathbf{f}}_a &= \mathbf{f}_a\odot\bigl(1+\alpha\,\mathbf{g}\bigr).
 \end{aligned}
 \end{equation}
 Here $\boldsymbol{\omega}$ are learnable logits over $\{LH,HL,HH\}$ and $\alpha$ controls the gate strength. $\mathrm{Gate}(\cdot)$ is a squeeze-excitation style channel MLP (two $1\times 1$ convs with sigmoid output) driven by pooled high-frequency energy. We then form $\widetilde{\mathbf{f}}_1=[\widetilde{\mathbf{f}}_a,\mathbf{f}_b]$ and apply it only to the box branch: high-frequency energy is a direct proxy for boundary sharpness and thus improves offset estimation, while leaving the classification stream unchanged avoids over-fitting to textures and background clutter.
 For the remaining levels $i>1$, we set $\widetilde{\mathbf{f}}_i=\mathbf{f}_i$.
 
 \noindent\textbf{Box/class prediction and decoding.}
 FDHead predicts per-location class logits and distributional box offsets (DFL) as
 \begin{equation}
 \label{eq:fdhead_pred}
 \mathbf{b}_i = \mathrm{Scale}_i\bigl(\mathcal{H}_{\mathrm{box}}(\widetilde{\mathbf{f}}_i)\bigr),\qquad
 \mathbf{p}_i = \mathcal{H}_{\mathrm{cls}}(\mathbf{f}_i),
 \end{equation}
 and decodes boxes by $\widehat{\mathbf{B}}=\mathrm{dist2bbox}(\mathrm{DFL}(\mathbf{b}),\mathbf{A})\cdot\mathbf{s}$ with anchors $\mathbf{A}$ and strides $\mathbf{s}$. This design targets small objects by frequency-gating only the finest level while keeping the remaining head computation shared and lightweight.

 %---------------------
 \section{Experiment}
 \subsection{Datasets and Metrics}
We evaluate our framework on four benchmarks to demonstrate its robustness and cross-domain generalization: VisDrone2019 \cite{du2019visdrone}, TinyPerson \cite{yu2020scale}, UAVDT \cite{du2018unmanned}, and DOTA v1 \cite{xia2018dota}. \textbf{VisDrone2019} is our primary benchmark and is particularly challenging due to dense small objects and severe scale variation, where most targets are smaller than $50 \times 50$ pixels.

We report both accuracy and efficiency, including mAP$_{50}$, the number of parameters, GFLOPs, model size, and FPS.

\subsection{Configuration}
The experimental configuration is detailed in Table \ref{tab:config}.

% --- 表格部分 (保持不变) ---
\begin{table}[ht]
    \centering
    \caption{\textbf{Configuration of Training and Testing Experiment Environments.} Detailed hardware and software configuration used for all experiments in this study.}
    \label{tab:config}
    \vspace{0.2cm} 
    
    \begin{tabular}{ll} 
        \toprule
        \textbf{Environment} & \textbf{Parameter} \\
        \midrule
        CPU              & Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz \\
        GPU              & NVIDIA A100-PCIE-40GB \\
        VRAM             & 40 GB \\
        RAM              & 46 GB \\
        Operating System & Rocky Linux 8.5 (Green Obsidian) \\
        Language         & Python 3.10.14 \\
        Frame            & PyTorch 2.1.0 \\
        CUDA Version     & 12.6 \\
        \bottomrule
    \end{tabular}
\end{table}
%这个表格需要调整大小/格式
For YOLO-style architectures, models are trained for 300 epochs with an input resolution of $640 \times 640$ and batch size 16, using SGD optimization. Unless otherwise specified, Mosaic augmentation is enabled throughout training; we use 4 dataloader workers and disable AMP.
%其他的架构还没写


%---------------------
\section{Main Results}

\subsection{Alation }


%---------------------
\section{Analyses and Discussion}




%---------------------
\section{Conclusion}










%---------------------
\bibliography{example_paper}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$
pages long. For the final version, one more page can be added. If you want, you
can use an appendix like this one.

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you
prefer a one-column appendix, or can be removed if you prefer a two-column
appendix.  Apart from this possible change, the style (font size, spacing,
margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
