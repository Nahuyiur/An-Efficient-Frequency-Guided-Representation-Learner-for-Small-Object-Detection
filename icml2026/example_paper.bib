@article{detr,
  title={End-to-End Object Detection with Transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  journal={arXiv preprint arXiv:2005.12872},
  year={2020}
}


@inproceedings{elan,
  title={Efficient Long-Range Attention Network for Image Super-resolution},
  author={Zhang, Xindong and Zeng, Hui and Guo, Shi and Zhang, Lei},
  booktitle={European Conference on Computer Vision},
  pages={649--667},
  year={2022},
  organization={Springer}
}

@article{yolov10,
  title={{YOLOv10}: Real-Time End-to-End Object Detection},
  author={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  journal={arXiv preprint arXiv:2405.14458},
  year={2024}
}

@article{yolov11,
  title={{YOLOv11}: An Overview of the Key Architectural Enhancements},
  author={Khanam, Rahima and Hussain, Muhammad},
  journal={arXiv preprint arXiv:2410.17725},
  year={2024}
}

@article{fbrt-yolo,
  title={{FBRT-YOLO}: Faster and Better for Real-Time Aerial Image Detection},
  author={Xiao, Yao and Xu, Tingfa and Xin, Yu and Li, Jianan},
  journal={arXiv preprint arXiv:2504.20670},
  year={2025}
}

@article{rt-detr,
  title={DETRs Beat YOLOs on Real-time Object Detection},
  author={Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
  journal={arXiv preprint arXiv:2304.08069},
  year={2024}
}

@article{dino,
  title={{DINO}: {DETR} with Improved DeNoising Anchor Boxes for End-to-End Object Detection},
  author={Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung},
  journal={arXiv preprint arXiv:2203.03605},
  year={2022}
}

@inproceedings{lsnet,
  title={{LSNet}: See Large, Focus Small},
  author={Wang, Ao and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={},
  year={2025}
}

@article{mobileuvit,
  title={{Mobile U-ViT}: Revisiting Large Kernel and {U}-Shaped ViT for Efficient Medical Image Segmentation},
  author={Tang, Fenghe and Nian, Bingkun and Ding, Jianrui and Ma, Wenxin and Quan, Quan and Dong, Chengqi and Yang, Jie and Liu, Wei and Zhou, S. Kevin},
  journal={arXiv preprint arXiv:2508.01064},
  year={2025}
}

@inproceedings{ssd,
  title={{SSD}: Single Shot MultiBox Detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I},
  pages={21--37},
  year={2016},
  publisher={Springer}
}

@inproceedings{fasterrcnn,
  title={Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={28},
  year={2015}
}

@article{hs-fpn,
  title={{HS-FPN}: High Frequency and Spatial Perception FPN for Tiny Object Detection},
  author={Shi, Zican and Hu, Jing and Ren, Jie and Ye, Hengkang and Yuan, Xuyang and Ouyang, Yan and He, Jia and Ji, Bo and Guo, Junyu},
  journal={arXiv preprint arXiv:2412.10116},
  year={2025}
}

@inproceedings{fpn,
  title={Feature Pyramid Networks for Object Detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2117--2125},
  year={2017}
}


@article{gfnet,
  title={{GFNet}: Global Filter Networks for Visual Recognition},
  author={Rao, Yongming and Zhao, Wenliang and Zhu, Zheng and Zhou, Jie and Lu, Jiwen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  volume={45},
  number={9},
  pages={10960--10973},
  year={2023},
  month={September},
  publisher={IEEE},
  doi={10.1109/TPAMI.2023.3263824}
}

@article{haar,
title = {Haar wavelet downsampling: A simple but effective downsampling module for semantic segmentation},
journal = {Pattern Recognition},
volume = {143},
pages = {109819},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109819},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005174},
author = {Guoping Xu and Wentao Liao and Xuan Zhang and Chang Li and Xinwei He and Xinglong Wu},
keywords = {Semantic segmentation, Downsampling, Haar wavelet, Information entropy},
abstract = {Downsampling operations such as max pooling or strided convolution are ubiquitously utilized in Convolutional Neural Networks (CNNs) to aggregate local features, enlarge receptive field, and minimize computational overhead. However, for a semantic segmentation task, pooling features over the local neighbourhood may result in the loss of important spatial information, which is conducive for pixel-wise predictions. To address this issue, we introduce a simple yet effective pooling operation called the Haar Wavelet-based Downsampling (HWD) module. This module can be easily integrated into CNNs to enhance the performance of semantic segmentation models. The core idea of HWD is to apply Haar wavelet transform for reducing the spatial resolution of feature maps while preserving as much information as possible. Furthermore, to investigate the benefits of HWD, we propose a novel metric, named as feature entropy index (FEI), which measures the degree of information uncertainty after downsampling in CNNs. Specifically, the FEI can be used to indicate the ability of downsampling methods to preserve essential information in semantic segmentation. Our comprehensive experiments demonstrate that the proposed HWD module could (1) effectively improve the segmentation performance across different modality image datasets with various CNN architectures, and (2) efficiently reduce information uncertainty compared to the conventional downsampling methods. Our implementation are available at https://github.com/apple1986/HWD.}
}

@article{fdconv,
  title={Frequency Dynamic Convolution for Dense Image Prediction},
  author={Chen, Linwei and Gu, Lin and Li, Liang and Yan, Chenggang and Fu, Ying},
  journal={arXiv preprint arXiv:2503.18783},
  year={2025}
}

@article{wavelet-transformer,
  author={Li, Xiaotong and Jiao, Licheng and Liu, Fang and Yang, Shuyuan and Zhu, Hao and Liu, Xu and Li, Lingling and Ma, Wenping},
  journal={IEEE Transactions on Multimedia}, 
  title={Adaptive Complex Wavelet Informed Transformer Operator}, 
  year={2025},
  volume={27},
  number={},
  pages={3513-3526},
  keywords={Transformers;Wavelet transforms;Kernel;Wavelet domain;Transforms;Visualization;Frequency-domain analysis;Computer vision;Spatial resolution;Decoding;Complex wavelet;multiscale frequency representation;neural operator;vision transformer},
  doi={10.1109/TMM.2025.3535392}}


@InProceedings{wtconv,
  author    = {Finder, Shahaf E. and Amoyal, Roy and Treister, Eran and Freifeld, Oren},
  editor    = {Leonardis, Ale{\v{s}} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  title     = {Wavelet Convolutions for Large Receptive Fields},
  booktitle = {Computer Vision -- ECCV 2024},
  year      = {2025},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {363--380},
  abstract  = {In recent years, there have been attempts to increase the kernel size of Convolutional Neural Nets (CNNs) to mimic the global receptive field of Vision Transformers' (ViTs) self-attention blocks. That approach, however, quickly hit an upper bound and saturated way before achieving a global receptive field. In this work, we demonstrate that by leveraging the Wavelet Transform (WT), it is, in fact, possible to obtain very large receptive fields without suffering from over-parameterization, e.g., for a $k \times k$ receptive field, the number of trainable parameters in the proposed method grows only logarithmically with k. The proposed layer, named WTConv, can be used as a drop-in replacement in existing architectures, results in an effective multi-frequency response, and scales gracefully with the size of the receptive field. We demonstrate the effectiveness of the WTConv layer within ConvNeXt and MobileNetV2 architectures for image classification, as well as backbones for downstream tasks, and show it yields additional properties such as robustness to image corruption and an increased response to shapes over textures. Our code is available at https://github.com/BGU-CS-VIL/WTConv.},
  isbn      = {978-3-031-72949-2}
}
